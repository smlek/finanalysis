---
title: "Machine Learning on Earnings Results"
subtitle: ""
author: "Stan Mlekodaj"
date: "February 29, 2016"
output: html_document
---
```{r overhead, echo=FALSE, message=FALSE}
# General environment setup
# Clear variables
rm(list=ls())
set.seed(18)
# all par settings which could be changed.
old.par <- par(no.readonly = TRUE)
# Load required packages
library(knitr)
library(xtable)
#library(data.table)
#library(nnet)
#library(neuralnet)
#library(randomForest)
#library(RSNNS)
#library(ROCR)
```
```{r, ref.label="functions", echo=FALSE}
#runs chunk with all function definitions
```

## To-Do's
- Different datasets for classification/regression models



## General flow

    1. Define data inputs, ouputs. Make sure outputs are ~50/50 outcomes (not skewed)
        a. Feature selection/elimination, PCA
    2. normalize data (except ouputs when binary)
        a. dimensionality reduction/PCA (optional) 
    3. Split data into training, cross-validation, test sets
    4. Train model on training set
    5. Tune model parameters for best cross-validation performance
    6. Evaluate model on test set
    

## Lessons learned

    
## Data Definitions

```{r,echo=FALSE}
############ define data, model formula (training sets in rows) #######
# data directory
dataDir<-paste(Sys.getenv("HOME"),"/Rscripts/data/",sep="")
# input data file
dataFile <- "earningsData_cleaned_Robj"
load(paste(dataDir,dataFile,sep=""))
rawData <- earnData

# discard if zero After Earnings Change
rawData <- rawData[rawData$ChgAftEarn != 0, ]
# define SignChgAftEarn = pos/neg After Earnings Change
#rawData$SignChgAftEarn <- sign(rawData$ChgAftEarn)
rawData$SignChgAftEarn <- 0.5 * (sign(rawData$ChgAftEarn) + 1)

#rawData$SignChgAftEarn <- factor(rawData$SignChgAftEarn, levels=c("1","-1"), labels=c("Up","Down"))

# model formula definition (neuralnet, nnet)
#fo <- formula("output ~ .")
# this approach ensures compatibility with neuralnet() which does not take "y ~ ."
n <- names(rawData)
fo <- as.formula(paste("SignChgAftEarn ~", 
                       paste(n[!n %in% c("ChgAftEarn","GapAftEarn","SignChgAftEarn")],
                             collapse = " + "
                             )
                       )
                 )

# input & output column indexes
inCols <- c(1:60)
outCols <- c(63)

# Which columns to normalize - 0/1 outputs/inputs dont need scaling.
normCols <- c(1:60)

#######################################################################

# ############ define data, model formula (training sets in rows) #######
# rawData <- infert
# 
# # model formula definition (neuralnet, nnet)
# fo <- formula("case ~ age + parity + induced + spontaneous")
# 
# # input & output column indexes
# inCols <- c(2, 3, 4, 6)
# outCols <- c(5)
# 
# # Which columns to normalize - 0/1 outputs/inputs dont need scaling.
# normCols <- c(2, 3, 4, 6, 7, 8)
# #######################################################################

nInputs <- nrow(rawData)
featureNames <- colnames(rawData)

# Feature Normalization
allData <- rawData
allData[ , normCols] <- normalizeData(rawData[ , normCols], type = "norm") #also type = "0_1" or "center"

# randomize rows
allData <- allData[sample(nInputs), ]

# split data in training, cross-validation, and test sets
trainBreak <- nInputs * 0.6 # 60% train
cvBreak <- nInputs * 0.8    # 20% cv, 20% test
trainData <- allData[1 : trainBreak, ]
cvData <- allData[(trainBreak + 1) : cvBreak, ]
testData <- allData[(cvBreak + 1) : nInputs, ]

# for RSNNS - split into input & output matrixes 
trainInputs <- trainData[, inCols]
trainOutputs <- trainData[, outCols]
cvInputs <- cvData[, inCols]
cvOutputs <- cvData[, outCols]
testInputs <- testData[, inCols]
testOutputs <- testData[, outCols]
```

Input Data Information:

- `r ncol(trainInputs)` input features
- `r ncol(trainOutputs)` outputs
- `r nInputs` total data sets

Prediction Formula:  `r fo`

## Decision Tree (rpart)

```{r,echo=FALSE}
library(caret)
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3,
                       #summaryFunction = twoClassSummary,
                       classProbs = TRUE
                       )

set.seed(1)
rpartModel <- train(fo, data = rbind(trainData, cvData), method = "rpart",
                   tuneLength = 30,
                   metric = "Accuracy",
                   trControl = cvCtrl
                   )
rpartModel
rpartPred2 <- predict(rpartModel, testData)
confusionMatrix(rpartPred2, testOutputs)


```

## GLM

```{r,echo=FALSE}

set.seed(1)
glmModel <- train(fo, data = rbind(trainData, cvData), method = "glm",
                  family = binomial(),
                  #tuneLength = 30,
                  metric = "Accuracy",
                  trControl = cvCtrl
                  )
glmModel
glmModelPredict <- predict(glmModel, testData)
confusionMatrix(glmModelPredict, testOutputs)
```

## Neural Network

```{r,echo=FALSE}
cvCtrl <- trainControl(method = "repeatedcv", repeats = 3,
                       #summaryFunction = twoClassSummary,
                       classProbs = TRUE
                       )
grid <- expand.grid(c(layer1 = c(10, 20),
                      layer2 = 0,
                      layer3 = 0
                      )
)

set.seed(1)
nnModel <- train(fo, data = rbind(trainData, cvData), method = "neuralnet",
                 #tuneLength = 30,
                 metric = "Accuracy",
                 trControl = cvCtrl,
                 tuneGrid = grid,
                 threshold = 0.01,
                 stepmax = 1e+5,
                 learningrate = 0.01,
                 lifesign = "full",
                 algorithm = "rprop+",
                 err.fct = "ce",
                 act.fct = "logistic",
                 linear.output = FALSE                
                 )


nnModel
nnModelPredict <- predict(nnModel, testData)
confusionMatrix(nnModelPredict, testOutputs)
```






```{r functions, echo=FALSE, eval=FALSE}
# Define functions in this chunk

##############  NEEDS VERIFICATION
# Takes predicted & target values, computes best prediction cutoff for max accuracy.
# ONLY FOR BINARY CLASSIFICATION - targets values must have only 2 values
# Returns cutoff and resulting accuracy. Also plots 4 metrics vs. cutoffs
getCutoffAndPlot <- function(predicted, targets) {
   
#     par(mfrow=c(1,1))
#     plotROC(predicted, targets, main = "ROC Curve") # ROC curve
    
    # make predicitons object using ROCR
    pred_obj <- ROCR::prediction(predicted, targets) # nerualnet also has prediction()
    # performance - need decision on most important metric
    perf_obj <- ROCR::performance(pred_obj, "acc", "cutoff")
    maxMetricInd <- which.max(perf_obj@y.values[[1]])    # index of metric max
    cutoff <- pred_obj@cutoffs[[1]][maxMetricInd]     # cutoff for max metric 
    perfMetric <- perf_obj@y.values[[1]][maxMetricInd]   # metric value at cutoff
    perfMetricName <- perf_obj@y.name                    # metric name
    
    # Plot several metrics vs. cutoff
    par(mfrow=c(2,2))
    plot(ROCR::performance(pred_obj, "acc", "cutoff"), ylim = c(0,1), main = "Accuracy")
    plot(ROCR::performance(pred_obj, "f", "cutoff"), ylim = c(0,1), main = "F-Score")
    plot(ROCR::performance(pred_obj, "phi", "cutoff"), ylim = c(-1,1), main = "Phi Corr. Coef.")
    plot(ROCR::performance(pred_obj, "sar", "cutoff"), main = "SAR Multi-Score")
    
    return(list(cutoff, perfMetricName, perfMetric))
}


```

